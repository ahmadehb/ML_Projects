# Project 2: Polynomial Regression & Gradient Descent

## Overview

This project explores **polynomial regression** using both **closed-form solutions** and **batch gradient descent**. It investigates how 
model complexity (polynomial degree) and optimization strategies impact performance and generalization.

The goal is to learn a regression function that predicts a continuous output `y` from a single numeric feature `x`, based on synthetic data.

## Purpose
This assignment was designed to deepen understanding of:
- Linear and nonlinear regression modeling
- Gradient descent as an optimization method
- Feature transformation for polynomial expansion
- Overfitting/underfitting trade-offs

---

## Dataset

- `regression_train.csv` and `regression_test.csv` are synthetic datasets containing `(x, y)` pairs:
  - `x`: a single real-valued input
  - `y`: a noisy target variable generated by an unknown nonlinear function

The dataset is intentionally small and simple to visually interpret bias-variance tradeoffs and optimization behavior.

---

## Concepts Covered

| Polynomial Regression | Fit models of degree 0â€“10 |
| Gradient Descent | Varying learning rates, convergence visualization |
| Feature Engineering | Custom implementation of polynomial feature expansion |
| Model Evaluation | RMSE on train/test sets |
| Overfitting Analysis | RMSE vs. polynomial degree |
| Visualization | Regression curves, error curves |

---

## ðŸ—‚Files

| `regression_train.csv` | Training set |
| `regression_test.csv` | Test set |

---
