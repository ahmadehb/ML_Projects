# Project 2: Polynomial Regression & Gradient Descent

## Overview

This project explores **polynomial regression** using both **closed-form solutions** and **batch gradient descent**. It investigates how 
model complexity (polynomial degree) and optimization strategies impact performance and generalization.

The goal is to learn a regression function that predicts a continuous output `y` from a single numeric feature `x`, based on synthetic data.

## Purpose
This assignment was designed to deepen understanding of:
- Linear and nonlinear regression modeling
- Gradient descent as an optimization method
- Feature transformation for polynomial expansion
- Overfitting/underfitting trade-offs

---

## Dataset

- `regression_train.csv` and `regression_test.csv` are synthetic datasets containing `(x, y)` pairs:
  - `x`: a single real-valued input
  - `y`: a noisy target variable generated by an unknown nonlinear function

The dataset is intentionally small and simple to visually interpret bias-variance tradeoffs and optimization behavior.

---

## Concepts Covered
| Concept | Description |
|--------|-------------|
| **Polynomial Regression** | Fit models of degree 0â€“10 |
| **Gradient Descent** | Varying learning rates, convergence visualization |
| **Feature Engineering** | Custom implementation of polynomial feature expansion |
| **Model Evaluation** | RMSE on train/test sets |
| **Overfitting Analysis** | RMSE vs. polynomial degree |
| **Visualization** | Regression curves, error curves |

---

## ðŸ—‚Files

| `regression_train.csv` | Training set |
| `regression_test.csv` | Test set |

---
